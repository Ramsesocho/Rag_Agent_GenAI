 Agent Conversationnel RAG ‚Äì Projet GenAI

Ce projet est une impl√©mentation compl√®te d‚Äôun agent conversationnel de type RAG (Retrieval-Augmented Generation) d√©velopp√© pour le cours **Generative AI (GenAI)** √† Aivancity.

---

##  Objectifs du projet

Ce projet r√©pond aux 6 consignes du devoir :

 | Exigence GenAI | R√©ponse dans ce projet |
- Choisir une base de connaissances | Des fichiers PDF ou TXT stock√©s localement (`/data`) |
- Indexation avec un mod√®le d'embedding | Mod√®le `all-MiniLM-L6-v2` (HuggingFace) |
- √âtape de retrieval | Recherche vectorielle avec **FAISS** |
- G√©n√©ration avec un LLM | **Mistral 7B**, ex√©cut√© en local via **Ollama** |
- Interface conversationnelle | Interface **Streamlit** permettant upload, r√©sum√© et Q/R |
 - √âvaluation (perf & impact) | √âvalu√© manuellement + architecture **100% locale = impact r√©duit** |

---

 Acc√®s au projet pour le professeur

### GitHub :  
üîó [https://github.com/Ramsesocho/Rag_Agent_GenAI](https://github.com/Ramsesocho/Rag_Agent_GenAI)

### Interface en local :

1. T√©l√©charger ou cloner le projet** :
   ```bash
   git clone https://github.com/Ramsesocho/Rag_Agent_GenAI.git
   cd Rag_Agent_GenAI
2. Activer l'environnement virtuel et installer les d√©pendances :

python -m venv env
source env/bin/activate  # Windows : .\env\Scripts\activate
pip install -r requirements.txt (fichier contenant toutes mes dependances)

3 . Installer et lancer Ollama avec Mistral :

brew install ollama              # si Homebrew est install√© sur votre ordi ou terminal sinon install√© homebrew avant
ollama pull mistral              # t√©l√©charge Mistral 7B
ollama serve                     # d√©marre Ollama localement

4. Lancer l‚Äôinterface de l‚Äôagent :
streamlit run app.py

Fonctionnalit√©s

  Upload de fichiers PDF ou TXT
  Indexation automatique dans FAISS
  Question/R√©ponse avec Mistral 7B
  R√©sum√© automatique des documents
  Historique des √©changes

Technologies utilis√©es

  Composant	Technologie
    Embedding	all-MiniLM-L6-v2 (HuggingFace)
    Vector DB	FAISS
    LLM	Mistral 7B via Ollama
    Orchestration	LangChain
    Interface	Streamlit
√âvaluation et impact environnemental

  Agent ex√©cut√© 100% en local
   Aucune d√©pendance √† OpenAI ou API cloud
   Pas de co√ªt d‚Äôinf√©rence externe
   Empreinte carbone r√©duite (pas d‚Äôappels sortants)


   
Arborescence du projet

rag-agent/
‚îú‚îÄ‚îÄ app.py                  # Interface Streamlit
‚îú‚îÄ‚îÄ rag_agent.py            # Backend RAG (retrieval + g√©n√©ration)
‚îú‚îÄ‚îÄ vectorstore/            # Index FAISS persistant
‚îú‚îÄ‚îÄ data/                   # Fichiers utilisateur upload√©s
‚îú‚îÄ‚îÄ requirements.txt        # D√©pendances Python
‚îú‚îÄ‚îÄ .gitignore              # Ignore .env, vectorstore, __pycache__
‚îî‚îÄ‚îÄ README.md               # Ce document


 R√©alis√© par

Rams√®s Appesse
 √âtudiant IA @ Aivancity Paris-Cachan
 Projet final du module Generative AI


