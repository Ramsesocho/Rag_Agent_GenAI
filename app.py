import os
import streamlit as st
from rag_agent import ask_agent
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings

# Configuration de la page
st.set_page_config(
    page_title="ğŸ§  Agent RAG Local",
    page_icon="ğŸ”",
    layout="centered",
)

st.title("ğŸ¤– Agent RAG - Mistral local (Ollama)")
st.markdown("Pose une question sur tes documents CANAL PLUS TELECOM vectorisÃ©s avec **Mistral** exÃ©cutÃ© localement via Ollama.")

# Initialiser lâ€™historique
if "history" not in st.session_state:
    st.session_state.history = []

# ğŸ“ Upload de fichier
st.subheader("ğŸ“‚ Importer un document")
uploaded_file = st.file_uploader("Uploader un fichier .pdf ou .txt", type=["pdf", "txt"])

if uploaded_file is not None:
    with st.spinner("ğŸ“š Lecture du document..."):
        # Sauvegarde dans /data
        os.makedirs("data", exist_ok=True)
        file_path = os.path.join("data", uploaded_file.name)

        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())

        # Loader adaptÃ©
        loader = PyPDFLoader(file_path) if file_path.endswith(".pdf") else TextLoader(file_path)
        docs = loader.load()
        full_text = "\n".join([doc.page_content for doc in docs])

        st.success("âœ… Document prÃªt Ã  Ãªtre vectorisÃ© ou rÃ©sumÃ©.")

        # ğŸ“„ Afficher le contenu brut en option
        with st.expander("ğŸ“ Voir le contenu extrait"):
            st.text_area("Contenu du document :", full_text, height=200)

        # â• Bouton de rÃ©sumÃ©
        if st.button("ğŸ§  RÃ©sumer ce document"):
            with st.spinner("RÃ©sumÃ© en cours..."):
                from langchain_community.llms import Ollama
                llm = Ollama(model="mistral")
                prompt = f"Voici un document :\n{full_text}\n\nFais-moi un rÃ©sumÃ© clair et structurÃ© en franÃ§ais."
                try:
                    summary = llm.invoke(prompt)
                    st.success("âœ… RÃ©sumÃ© gÃ©nÃ©rÃ© :")
                    st.markdown(summary)
                except Exception as e:
                    st.error(f"Erreur lors du rÃ©sumÃ© : {e}")

        # â• Bouton vectorisation
        if st.button("ğŸ“¡ Ajouter ce document Ã  la base de connaissances"):
            with st.spinner("DÃ©coupage & vectorisation..."):
                from langchain.text_splitter import CharacterTextSplitter
                from langchain_community.vectorstores import FAISS
                from langchain_huggingface import HuggingFaceEmbeddings

                splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                chunks = splitter.split_documents(docs)

                embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
                vectorstore = FAISS.load_local("vectorstore", embedding_model, allow_dangerous_deserialization=True)
                vectorstore.add_documents(chunks)
                vectorstore.save_local("vectorstore")

                st.success("âœ… Document vectorisÃ© et ajoutÃ© avec succÃ¨s !")

# ğŸ’¬ Question utilisateur
question = st.text_input("ğŸ’¬ Pose ta question :", placeholder="Ex: Quels sont les avantages de la fibre optique ?")

# Envoi de la question
if st.button("ğŸ§  Interroger l'agent") and question.strip():
    with st.spinner("RÃ©flexion en cours..."):
        try:
            response = ask_agent(question)
            st.session_state.history.append({"question": question, "response": response})
            st.success("âœ… RÃ©ponse gÃ©nÃ©rÃ©e !")
        except Exception as e:
            st.error(f"âŒ Une erreur est survenue : {e}")

# ğŸ—‚ï¸ Historique des Ã©changes
if st.session_state.history:
    st.markdown("---")
    st.subheader("ğŸ“œ Historique des interactions")
    for i, entry in enumerate(reversed(st.session_state.history), 1):
        st.markdown(f"**{i}. Question :** {entry['question']}")
        st.markdown(f"ğŸ§  **RÃ©ponse :** {entry['response']}")
        st.markdown("---")
